# Distributed Training with Kubeflow Pipeline

[![Kubernetes](https://img.shields.io/badge/Kubernetes-v1.34.0-blue)](https://kubernetes.io/)
[![Kubeflow](https://img.shields.io/badge/Kubeflow-v1.8.1-orange)](https://www.kubeflow.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0.1-red)](https://pytorch.org/)

**Authors:** Zahra ABOUHANE, Yassine BAZGOUR  
**Supervisor:** Dr. Fahd KALLOUBI  
**Institution:** Faculty of Sciences Semlalia, Department of Computer Science  
**Academic Year:** 2024-2025

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [What Was Implemented](#what-was-implemented)
- [Prerequisites](#prerequisites)
- [Project Structure](#project-structure)
- [Installation Guide](#installation-guide)
- [Usage](#usage)
- [Results](#results)
- [Troubleshooting](#troubleshooting)
- [Acknowledgments](#acknowledgments)

---

## ğŸ¯ Overview

This project implements an **automated ML training pipeline** using Kubernetes and Kubeflow on a local Minikube cluster. It demonstrates MLOps workflow automation from data ingestion through model deployment.

### What This Does

- Uploads CIFAR-10 dataset to MinIO object storage
- Applies data preprocessing and augmentation
- Trains ResNet-18 with Kubeflow PyTorchJob achieving **77.11% test accuracy**
- Tracks all experiments with MLflow
- Automates entire workflow with Argo Workflows (4-step pipeline)
- Deploys model serving infrastructure with KServe

### Technologies Used

| Component | Tool | What It Does |
|-----------|------|--------------|
| **Cluster** | Minikube | Local Kubernetes (1 node, 8GB RAM, 4 CPUs) |
| **Storage** | MinIO | Stores datasets, models, and artifacts |
| **Training** | Kubeflow Training Operator | Manages PyTorchJob execution |
| **Tracking** | MLflow | Logs metrics, parameters, and artifacts |
| **Orchestration** | Argo Workflows | Automates pipeline execution |
| **Serving** | KServe + Knative + Istio | Model inference infrastructure |
| **ML Framework** | PyTorch 2.0.1 | Deep learning training |

**Deployment:** 7 Kubernetes components, 25+ pods across 8 namespaces

---

## ğŸ—ï¸ Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      User / Client Application      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SERVING: KServe + Knative + Istio  â”‚
â”‚  â””â”€ InferenceService (REST API)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ORCHESTRATION: Argo Workflows      â”‚
â”‚  â””â”€ 4-step automated pipeline       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TRAINING: Kubeflow PyTorchJob      â”‚
â”‚  â””â”€ ResNet-18 on CIFAR-10          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TRACKING:       â”‚  â”‚  DATA PIPELINE:  â”‚
â”‚  MLflow          â”‚  â”‚  Preprocessing   â”‚
â”‚  â””â”€ Experiments  â”‚  â”‚  â””â”€ Augmentation â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STORAGE: MinIO (S3-compatible)     â”‚
â”‚  â””â”€ Buckets: raw-data, processed,   â”‚
â”‚     models, mlflow artifacts        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INFRASTRUCTURE: Kubernetes Cluster â”‚
â”‚  â””â”€ Minikube (8GB RAM, 4 CPUs)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Workflow:**
1. Data uploaded to MinIO (`raw-data/`)
2. Feature engineering â†’ MinIO (`processed-data/`)
3. Argo Workflow triggers PyTorchJob
4. Training logs to MLflow, saves model to MinIO (`models/`)
5. KServe loads model from MinIO for inference

---

## âœ¨ What Was Implemented

### Data Pipeline
- âœ… CIFAR-10 dataset download (50K train, 10K test images)
- âœ… Upload to MinIO: `raw-data/cifar10-dataset.tar.gz` (162 MB)
- âœ… Feature engineering transformations:
  - RandomHorizontalFlip
  - RandomCrop (32x32, padding=4)
  - Normalization (mean=[0.49, 0.48, 0.45], std=[0.20, 0.20, 0.20])
- âœ… Processed data stored: `processed-data/cifar10-processed.tar.gz` (162 MB)

### Training
- âœ… Model: ResNet-18 (modified for 10 classes)
- âœ… Kubeflow PyTorchJob with **single worker** (CPU-only)
- âœ… Configuration:
  - Epochs: 10
  - Batch size: 32
  - Optimizer: SGD (lr=0.01, momentum=0.9, weight_decay=5e-4)
  - Scheduler: Cosine Annealing
- âœ… MLflow integration: logs loss and accuracy per epoch
- âœ… Model saved to MinIO: `models/resnet18_cifar10.pth` (44.7 MB)

### Automation
- âœ… Argo Workflow: 4 automated steps
  1. Data Verification (checks MinIO for processed data)
  2. Feature Engineering Check (validates preprocessing)
  3. Training Execution (submits PyTorchJob, waits for completion)
  4. Model Verification (confirms model saved)
- âœ… Single command execution: `kubectl create -f real_pipeline.yaml`
- âœ… Total pipeline duration: ~173 minutes

### Serving Infrastructure
- âœ… KServe InferenceService deployed
- âœ… Knative Serving (serverless autoscaling)
- âœ… Istio service mesh (traffic management)
- âœ… Flask REST API endpoints:
  - `POST /v1/models/cifar10:predict` (inference)
  - `GET /v1/models/cifar10` (health check)
- âœ… Model loads from MinIO on pod startup
- âš ï¸ Status: Deployed (resource-constrained during initialization)

### Results Achieved
- âœ… **Test Accuracy:** 77.11%
- âœ… **Test Loss:** 1.0313
- âœ… **Training Time:** 173 minutes on CPU
- âœ… All 7 components integrated successfully
- âœ… End-to-end automation working

---

## ğŸ¯ Implementation Details

### Training Configuration

**Current Setup (What We Used):**
- **Compute:** CPU-only (4 cores)
- **Workers:** Single worker
- **Training Time:** 173 minutes for 10 epochs
- **Resource Limits:** 2 CPUs, 3GB RAM per pod

**Note on GPU and Multi-Worker:**
The same PyTorchJob manifest supports:
- **GPU Training:** Change `resources.limits` to include `nvidia.com/gpu: 1`
- **Multi-Worker Distributed:** Increase `replicas` in PyTorchJob spec
- With GPU + multi-worker: training would complete in 15-30 minutes

We used single-worker CPU configuration due to:
- Development environment constraints (Minikube)
- CPU-only hardware availability
- Network complexity in single-node cluster

**The architecture supports distributed training** - only hardware/environment differs.

---

## ğŸ“¦ Prerequisites

### Required Software

**Installation via Command Line (Windows):**

| Tool | Version | Installation Command |
|------|---------|---------------------|
| **Minikube** | v1.37.0+ | `choco install minikube` |
| **kubectl** | v1.34.0+ | `choco install kubernetes-cli` |
| **Docker Desktop** | 20.10+ | Download from [docker.com](https://docs.docker.com/get-docker/) |
| **Python** | 3.10+ | `choco install python` |
| **Git** | Latest | `choco install git` |

**Verify Installation:**
```bash
minikube version    # Should show v1.37.0+
kubectl version --client    # Should show v1.34.0+
docker --version    # Should show 20.10+
python --version    # Should show 3.10+
git --version
```

### System Requirements

| Resource | Minimum | Used in This Project |
|----------|---------|---------------------|
| **RAM** | 8 GB | 8 GB |
| **CPU** | 4 cores | 4 cores |
| **Disk** | 20 GB free | 20 GB |

### Windows WSL2 Configuration

**Create this file on your system:**

**Location:** `C:\Users\<YourUsername>\.wslconfig`

**Content:**
```ini
[wsl2]
memory=10GB
processors=4
swap=2GB
```

**Apply changes:**
```bash
wsl --shutdown
# Then restart Docker Desktop
```

---

## ğŸ“ Project Structure
```
distributed-training-kubeflow-pipeline/
â”‚
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”‚
â”œâ”€â”€ infrastructure/                    # Kubernetes manifests
â”‚   â”œâ”€â”€ minio-deployment.yaml          # MinIO deployment + PVC + Service
â”‚   â”œâ”€â”€ mlflow-deployment.yaml         # MLflow server + PVC + Service
â”‚   â””â”€â”€ kserve-cert.yaml              # KServe TLS certificates
â”‚
â”œâ”€â”€ data/                              # Data pipeline scripts
â”‚   â”œâ”€â”€ upload_dataset.py              # Downloads CIFAR-10, uploads to MinIO
â”‚   â””â”€â”€ feature_engineering.py         # Applies transformations, uploads processed data
â”‚
â”œâ”€â”€ training/                          # Training components
â”‚   â”œâ”€â”€ train_distributed.py           # PyTorch training script with MLflow
â”‚   â”œâ”€â”€ pytorchjob.yaml               # Kubeflow PyTorchJob manifest
â”‚   â”œâ”€â”€ Dockerfile                    # Training container image
â”‚   â””â”€â”€ requirements.txt              # Training Python dependencies
â”‚
â”œâ”€â”€ pipelines/                         # Workflow orchestration
â”‚   â”œâ”€â”€ real_pipeline.yaml            # Argo: 4-step production workflow
â”‚   â”œâ”€â”€ ml_pipeline.py                # KFP SDK: pipeline definition (optional)
â”‚   â””â”€â”€ cifar10_argo_pipeline.yaml    # Argo: demo workflow
â”‚
â””â”€â”€ serving/                           # Model serving
    â”œâ”€â”€ serve.py                      # Flask inference server
    â”œâ”€â”€ Dockerfile.serving            # Serving container (pre-built PyTorch)
    â””â”€â”€ inference-service-prebuilt.yaml # KServe InferenceService manifest
```

---

## ğŸš€ Installation Guide

### Step 1: Clone Repository
```bash
git clone https://github.com/ZahraAbouhane/distributed-training-kubeflow-pipeline.git
cd distributed-training-kubeflow-pipeline
```

### Step 2: Start Kubernetes Cluster
```bash
minikube start --memory=8000 --cpus=4 --disk-size=20g

# Verify cluster is running
kubectl cluster-info
kubectl get nodes
```

**Expected output:**
```
NAME       STATUS   ROLES           AGE   VERSION
minikube   Ready    control-plane   30s   v1.34.0
```

### Step 3: Deploy Infrastructure Components

#### 3.1 MinIO - Object Storage
```bash
kubectl create namespace minio
kubectl apply -f infrastructure/minio-deployment.yaml
kubectl wait --for=condition=ready pod -l app=minio -n minio --timeout=300s

# Access MinIO console
minikube service minio -n minio --url
# Credentials: minioadmin / minioadmin
```

**What it deploys:**
- MinIO server with 10GB PersistentVolume
- Creates 4 buckets: `raw-data`, `processed-data`, `models`, `mlflow`
- Exposes ports: 9000 (API), 9001 (Console)

**File:** `infrastructure/minio-deployment.yaml`

#### 3.2 MLflow - Experiment Tracking
```bash
kubectl create namespace mlflow
kubectl apply -f infrastructure/mlflow-deployment.yaml
kubectl wait --for=condition=ready pod -l app=mlflow -n mlflow --timeout=300s

# Access MLflow UI
minikube service mlflow -n mlflow --url
```

**What it deploys:**
- MLflow tracking server
- SQLite backend for metadata
- MinIO S3 for artifact storage
- Exposes port: 5000

**File:** `infrastructure/mlflow-deployment.yaml`

#### 3.3 Kubeflow Training Operator
```bash
kubectl create namespace kubeflow
kubectl apply -k "github.com/kubeflow/training-operator/manifests/overlays/standalone?ref=v1.8.1"

# Verify installation
kubectl get pods -n kubeflow
kubectl get crd | grep pytorchjobs
```

**Expected output:**
```
pytorchjobs.kubeflow.org   2024-12-31T10:00:00Z
```

**What it deploys:**
- PyTorchJob Custom Resource Definition (CRD)
- Training Operator controller
- Watches for PyTorchJob resources

#### 3.4 Argo Workflows
```bash
kubectl create namespace argo
kubectl apply -n argo -f https://github.com/argoproj/argo-workflows/releases/download/v3.5.5/install.yaml
kubectl wait --for=condition=ready pod -l app=workflow-controller -n argo --timeout=300s

# Configure RBAC permissions
kubectl create clusterrolebinding argo-admin \
  --clusterrole=cluster-admin \
  --serviceaccount=argo:argo
```

**What it deploys:**
- Argo Workflow controller
- Argo Server (UI)
- Workflow CRDs

#### 3.5 KServe - Model Serving Stack
```bash
# Install cert-manager (TLS certificates)
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.3/cert-manager.yaml
kubectl wait --for=condition=ready pod -l app.kubernetes.io/instance=cert-manager -n cert-manager --timeout=300s

# Install Knative Serving (serverless)
kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.11.0/serving-crds.yaml
kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.11.0/serving-core.yaml

# Install Istio (service mesh)
kubectl create namespace istio-system
kubectl apply -f https://github.com/knative-sandbox/net-istio/releases/download/knative-v1.11.0/istio.yaml

# Install Knative networking with Istio
kubectl apply -f https://github.com/knative/net-istio/releases/download/knative-v1.11.0/net-istio.yaml

# Install KServe
kubectl apply -f https://github.com/kserve/kserve/releases/download/v0.11.2/kserve.yaml

# Create TLS certificates for KServe webhooks
kubectl apply -f infrastructure/kserve-cert.yaml

# Wait for KServe controller
kubectl wait --for=condition=ready pod -l control-plane=kserve-controller-manager -n kserve --timeout=300s
```

**What it deploys:**
- cert-manager (3 pods): Certificate management
- Knative Serving (6 pods): Serverless platform
- Istio (2 pods): Service mesh
- KServe (1 pod): Model serving controller

**File:** `infrastructure/kserve-cert.yaml`

### Step 4: Verify Installation
```bash
# Check all namespaces
kubectl get pods --all-namespaces | grep -E "minio|mlflow|kubeflow|argo|kserve|cert-manager|istio|knative"

# Expected: All pods showing Running status
```

**Checklist:**
- [ ] MinIO: 1/1 Running
- [ ] MLflow: 1/1 Running
- [ ] Training Operator: 1/1 Running
- [ ] Argo Controller: 1/1 Running
- [ ] KServe Controller: 2/2 Running
- [ ] cert-manager pods: 3 Running
- [ ] Istio pods: 2 Running
- [ ] Knative pods: 6 Running

---

## ğŸ’» Usage

### Phase 1: Data Preparation

#### Step 1: Install Python Dependencies
```bash
pip install -r requirements.txt
```

**File:** `requirements.txt` contains: `torch`, `torchvision`, `minio`, `mlflow`

#### Step 2: Upload CIFAR-10 Dataset
```bash
python data/upload_dataset.py
```

**What happens:**
1. Downloads CIFAR-10 dataset (178 MB)
2. Creates tar.gz archive
3. Uploads to MinIO bucket: `raw-data/cifar10-dataset.tar.gz`

**Output:**
```
Downloading CIFAR-10...
Creating archive...
Uploading to MinIO...
âœ… Uploaded to MinIO: raw-data/cifar10-dataset.tar.gz (162 MB)
```

**File:** `data/upload_dataset.py`

#### Step 3: Feature Engineering
```bash
python data/feature_engineering.py
```

**What happens:**
1. Downloads raw data from MinIO
2. Applies transformations:
   - Training: RandomHorizontalFlip + RandomCrop + ToTensor + Normalize
   - Testing: ToTensor + Normalize
3. Creates processed archive
4. Uploads to MinIO: `processed-data/cifar10-processed.tar.gz`

**Output:**
```
Processing 50,000 training images...
Processing 10,000 test images...
Applying augmentation...
âœ… Uploaded to MinIO: processed-data/cifar10-processed.tar.gz (162 MB)
```

**File:** `data/feature_engineering.py`

---

### Phase 2: Model Training

#### Step 1: Build Training Docker Image
```bash
cd training/

# Point Docker to Minikube's Docker daemon
eval $(minikube docker-env)  # Linux/Mac
# Windows PowerShell: minikube docker-env | Invoke-Expression

docker build -t pytorch-distributed-training:latest .

cd ..
```

**What happens:**
1. Uses `training/Dockerfile`
2. Installs PyTorch 2.0.1, torchvision, MLflow, MinIO client
3. Copies `train_distributed.py` into image
4. Image size: ~2 GB

**Files:**
- `training/Dockerfile`
- `training/requirements.txt`

#### Step 2: Submit PyTorchJob
```bash
kubectl apply -f training/pytorchjob.yaml

# Monitor training job
kubectl get pytorchjob -w
```

**Expected output:**
```
NAME                    STATE       AGE
pytorch-single-cifar10  Running     10s
pytorch-single-cifar10  Succeeded   173m
```

**Check training logs:**
```bash
# Get pod name
kubectl get pods | grep pytorch-single

# View logs
kubectl logs -f <pod-name>
```

**What happens during training:**
1. Pod starts with `pytorch-distributed-training:latest` image
2. Downloads processed data from MinIO
3. Initializes ResNet-18 model
4. Trains for 10 epochs:
   - Epoch 1: Loss 1.65, Acc 43.65%
   - Epoch 5: Loss 0.63, Acc 78.21%
   - Epoch 10: Loss 0.05, Acc 98.63%
5. Logs metrics to MLflow after each epoch
6. Evaluates on test set: **77.11% accuracy**
7. Saves model to MinIO: `models/resnet18_cifar10.pth`

**Duration:** ~173 minutes on CPU

**Files:**
- `training/pytorchjob.yaml` - Kubeflow job specification
- `training/train_distributed.py` - Training script with MLflow integration

**View in MLflow:**
```bash
minikube service mlflow -n mlflow --url
# Open URL in browser â†’ Experiments â†’ cifar10-distributed-training
```

---

### Phase 3: Automated Pipeline Execution

#### Execute Complete End-to-End Pipeline
```bash
kubectl create -f pipelines/real_pipeline.yaml

# Monitor workflow
kubectl get workflows -n argo -w
```

**Expected output:**
```
NAME                        STATUS      AGE
cifar10-real-pipeline-*     Running     30s
cifar10-real-pipeline-*     Succeeded   173m
```

**Check workflow details:**
```bash
kubectl describe workflow <workflow-name> -n argo
```

**View step logs:**
```bash
kubectl logs -n argo -l workflows.argoproj.io/workflow=<workflow-name> --all-containers=true
```

**Pipeline Steps Executed:**

**Step 1: Data Verification**
```
Checking MinIO for processed data...
âœ… Processed data found: processed-data/cifar10-processed.tar.gz (162 MB)
```

**Step 2: Feature Engineering Check**
```
Validating preprocessing metadata...
âœ… Feature engineering completed
```

**Step 3: Training Execution**
```
Submitting PyTorchJob: pipeline-training-job
Waiting for training completion...
[... 173 minutes of training ...]
âœ… Training completed successfully!
âœ… Test Accuracy: 77.11%
```

**Step 4: Model Verification**
```
Checking model in MinIO...
âœ… Model saved successfully: models/resnet18_cifar10.pth (44.7 MB)
ğŸ‰ PIPELINE COMPLETE!
```

**Total Duration:** ~173 minutes

**Files:**
- `pipelines/real_pipeline.yaml` - Production Argo Workflow (4 steps)
- `pipelines/ml_pipeline.py` - Kubeflow Pipelines SDK definition (optional alternative)
- `pipelines/cifar10_argo_pipeline.yaml` - Demo workflow (simpler, for testing)

---

### Phase 4: Model Serving

#### Step 1: Build Inference Docker Image
```bash
cd serving/

eval $(minikube docker-env)
docker build -f Dockerfile.serving -t cifar10-inference:latest .

cd ..
```

**What this does:**
- Pre-installs PyTorch, torchvision, Flask, MinIO client
- Copies `serve.py` (Flask REST API server)
- Avoids runtime pip installation (faster startup)
- Image size: ~2 GB

**Files:**
- `serving/Dockerfile.serving`
- `serving/serve.py`

#### Step 2: Deploy InferenceService
```bash
kubectl apply -f serving/inference-service-prebuilt.yaml

# Monitor deployment
kubectl get inferenceservice
kubectl get pods -l serving.kserve.io/inferenceservice=cifar10-classifier -w
```

**Expected status:**
```
NAME                 READY   URL
cifar10-classifier   True    http://cifar10-classifier.default.example.com
```

**What happens:**
1. KServe creates Knative Service
2. Knative creates pod with inference container
3. Pod downloads model from MinIO on startup
4. Flask server starts on port 8080
5. Istio configures routing

**Note:** In our environment, pod may show `1/2 Running` due to resource constraints during initialization.

**File:** `serving/inference-service-prebuilt.yaml`

#### Step 3: Test Inference (When Ready)
```bash
# Get service URL
SERVICE_URL=$(kubectl get inferenceservice cifar10-classifier -o jsonpath='{.status.url}')

# Test prediction with image
curl -X POST $SERVICE_URL/v1/models/cifar10:predict \
  -F "image=@test_airplane.jpg"
```

**Expected response:**
```json
{
  "predictions": [{
    "class": "airplane",
    "confidence": 0.89,
    "probabilities": {
      "airplane": 0.89,
      "automobile": 0.05,
      "bird": 0.02,
      "cat": 0.01,
      "deer": 0.01,
      "dog": 0.01,
      "frog": 0.00,
      "horse": 0.01,
      "ship": 0.00,
      "truck": 0.00
    }
  }]
}
```

**Health check:**
```bash
curl $SERVICE_URL/v1/models/cifar10
```

**Response:**
```json
{
  "name": "cifar10",
  "ready": true
}
```

---

## ğŸ“Š Results

### Training Performance

| Metric | Value |
|--------|-------|
| **Test Accuracy** | **77.11%** |
| **Test Loss** | 1.0313 |
| **Training Time** | 173 minutes (CPU) |
| **Model Size** | 44.7 MB |
| **Parameters** | ~11M (ResNet-18) |
| **CIFAR-10 Baseline** | 75-80% (competitive) |

### Training Progress by Epoch

| Epoch | Train Loss | Train Accuracy | Learning Rate |
|-------|-----------|----------------|---------------|
| 1 | 1.6529 | 43.65% | 0.0100 |
| 2 | 1.1466 | 59.99% | 0.0088 |
| 3 | 0.9321 | 67.77% | 0.0075 |
| 4 | 0.7698 | 73.39% | 0.0063 |
| 5 | 0.6306 | 78.21% | 0.0050 |
| 6 | 0.4982 | 82.61% | 0.0038 |
| 7 | 0.3526 | 87.78% | 0.0025 |
| 8 | 0.2065 | 92.86% | 0.0013 |
| 9 | 0.0986 | 96.75% | 0.0003 |
| 10 | 0.0467 | 98.63% | 0.0000 |

**Final Test:** 77.11% accuracy, 1.0313 loss

### Infrastructure Resource Usage

| Component | Pods | CPU Used | Memory Used | Storage |
|-----------|------|----------|-------------|---------|
| **MinIO** | 1 | 0.5 cores | 512 MB | 419 MB data |
| **MLflow** | 1 | 0.3 cores | 512 MB | 50 MB artifacts |
| **Training Operator** | 1 | 0.2 cores | 256 MB | - |
| **Argo Workflows** | 2 | 0.3 cores | 512 MB | - |
| **cert-manager** | 3 | 0.2 cores | 384 MB | - |
| **Istio** | 2 | 0.4 cores | 768 MB | - |
| **Knative Serving** | 6 | 0.6 cores | 1.5 GB | - |
| **KServe** | 1 | 0.2 cores | 512 MB | - |
| **Training (active)** | 1 | 1-2 cores | 2-3 GB | - |
| **TOTAL** | **25+** | **3-4 cores** | **5.8-6.8 GB** | **~470 MB** |

### Pipeline Execution Metrics

| Metric | Value |
|--------|-------|
| **Total Steps** | 4 |
| **Workflow Status** | âœ… Succeeded |
| **Total Duration** | ~173 minutes |
| **Data Verified** | âœ… 162 MB |
| **Model Saved** | âœ… 44.7 MB |
| **Automation** | Single command |

### Storage Breakdown

| Bucket | Contents | Size |
|--------|----------|------|
| `raw-data/` | cifar10-dataset.tar.gz | 162 MB |
| `processed-data/` | cifar10-processed.tar.gz | 162 MB |
| `models/` | resnet18_cifar10.pth | 44.7 MB |
| `mlflow/` | Experiment artifacts | ~50 MB |
| **Total** | | **~419 MB** |

---

## ğŸ› Troubleshooting

### Common Issues and Solutions

### 0. Permission Errors

**Symptom:**
```
Access denied
Permission denied
```

**Solution:**

**For Docker/Minikube issues:**
1. Ensure Docker Desktop is running
2. Make sure you're in `docker-users` group:
   - Search "Computer Management"
   - Local Users and Groups â†’ Groups â†’ docker-users
   - Add your user account
   - **Restart computer**

**For WSL2 configuration:**
If you can't create `.wslconfig`:
1. Right-click Notepad â†’ Run as Administrator
2. Create file: `C:\Users\<YourUsername>\.wslconfig`
3. Paste configuration
4. Save and close

**For kubectl/minikube commands:**
These should NOT require admin. If they do:
1. Reinstall Minikube to user directory (not Program Files)
2. Or run: `minikube config set WantUpdateNotification false`

#### 1. Pod Stuck in Pending

**Symptom:**
```bash
kubectl get pods
# NAME           READY   STATUS    AGE
# training-pod   0/1     Pending   5m
```

**Diagnosis:**
```bash
kubectl describe pod <pod-name>
# Events:
#   Warning  FailedScheduling  Insufficient cpu
```

**Solution:**
```bash
# Option 1: Scale down other services temporarily
kubectl scale deployment mlflow -n mlflow --replicas=0
kubectl scale deployment argo-server -n argo --replicas=0

# Option 2: Reduce resource requests in YAML
# Edit pytorchjob.yaml and change:
# resources.requests.cpu: "1" â†’ "500m"
```

#### 2. ImagePullBackOff Error

**Symptom:**
```bash
# STATUS: ImagePullBackOff or ErrImagePull
```

**Diagnosis:**
```bash
kubectl describe pod <pod-name>
# Events:
#   Warning  Failed  Failed to pull image
```

**Solution:**
```bash
# Ensure imagePullPolicy: Never in YAML
# Rebuild image in Minikube's Docker
eval $(minikube docker-env)
docker build -t <image-name>:latest .

# Verify image exists
docker images | grep <image-name>
```

#### 3. Training Job Fails

**Diagnosis:**
```bash
kubectl logs <training-pod-name>
```

**Common causes and solutions:**

**MinIO connection failed:**
```bash
# Check MinIO service running
kubectl get pods -n minio
kubectl get svc -n minio

# Test connectivity from another pod
kubectl run -it --rm debug --image=busybox --restart=Never -- \
  wget -O- http://minio.minio.svc.cluster.local:9000
```

**MLflow unreachable:**
```bash
# Check MLflow service
kubectl get pods -n mlflow
kubectl get svc -n mlflow
```

**Out of memory:**
```bash
# Reduce batch size in pytorchjob.yaml or train_distributed.py
# Change: BATCH_SIZE = 32 â†’ 16
```

#### 4. KServe Pod CrashLoopBackOff

**Diagnosis:**
```bash
kubectl logs <pod-name> -c kserve-container --previous
```

**Common causes:**

**Network isolation (Istio blocking pip):**
```
ERROR: Could not find a version that satisfies the requirement torch
```
**Solution:** Use pre-built image (already done in `inference-service-prebuilt.yaml`)

**Insufficient memory:**
```
Killed
```
**Solution:**
```bash
# Increase memory limit in inference-service-prebuilt.yaml
# resources.limits.memory: "1.5Gi" â†’ "2Gi"
```

**Model download failed:**
```
Error: Object does not exist
```
**Solution:**
```bash
# Verify model exists in MinIO
kubectl exec -n minio deployment/minio -- ls -lh /data/models/
```

#### 5. Argo Workflow Permission Errors

**Symptom:**
```
workflows.argoproj.io is forbidden
```

**Solution:**
```bash
# Create RBAC binding
kubectl create clusterrolebinding argo-admin \
  --clusterrole=cluster-admin \
  --serviceaccount=argo:argo

# Verify
kubectl get clusterrolebinding | grep argo
```

#### 6. Workflow Stuck in Running

**Diagnosis:**
```bash
kubectl describe workflow <workflow-name> -n argo
# Check Events and Conditions sections
```

**Common cause:** Step waiting for PyTorchJob

**Solution:**
```bash
# Check PyTorchJob status
kubectl get pytorchjob
kubectl describe pytorchjob pipeline-training-job

# Check training pod logs
kubectl get pods | grep pipeline-training
kubectl logs -f <pod-name>
```

### Useful Debug Commands
```bash
# View all resources across namespaces
kubectl get all --all-namespaces

# Check pod logs
kubectl logs <pod-name> -n <namespace>
kubectl logs <pod-name> -c <container-name>  # For multi-container pods
kubectl logs <pod-name> --previous  # Logs from crashed container

# Describe resource (shows events)
kubectl describe <resource-type> <name> -n <namespace>

# Get recent events
kubectl get events --sort-by='.lastTimestamp' -n <namespace>

# Execute into running pod
kubectl exec -it <pod-name> -n <namespace> -- /bin/bash

# Check resource usage
kubectl top pods -n <namespace>
kubectl top nodes

# Port forward to service
kubectl port-forward -n <namespace> svc/<service-name> <local-port>:<service-port>

# Check service endpoints
kubectl get endpoints -n <namespace>

# Restart deployment
kubectl rollout restart deployment <name> -n <namespace>

# Delete stuck pod
kubectl delete pod <pod-name> -n <namespace> --force --grace-period=0
```

### Getting Help

- **Kubernetes Issues:** https://kubernetes.io/docs/tasks/debug/
- **Kubeflow Training:** https://www.kubeflow.org/docs/components/training/
- **Argo Workflows:** https://argoproj.github.io/argo-workflows/
- **MLflow:** https://www.mlflow.org/docs/latest/index.html
- **KServe:** https://kserve.github.io/website/

---

## ğŸ™ Acknowledgments

- **Supervisor:** Dr. Fahd KALLOUBI for guidance and support throughout the project
- **Institution:** Faculty of Sciences Semlalia, Department of Computer Science for providing resources
- **Open Source Communities:** 
  - Kubernetes project for container orchestration
  - Kubeflow community for ML workflow tools
  - PyTorch team for deep learning framework
  - Argo project for workflow automation
  - MLflow developers for experiment tracking
  - MinIO team for object storage
  - KServe, Knative, and Istio communities for serving infrastructure

---

## ğŸ“š References

### Documentation
1. **Kubernetes Documentation:** https://kubernetes.io/docs/
2. **Kubeflow Training Operator:** https://www.kubeflow.org/docs/components/training/
3. **Argo Workflows:** https://argoproj.github.io/workflows/
4. **MLflow:** https://www.mlflow.org/docs/
5. **KServe:** https://kserve.github.io/website/
6. **PyTorch:** https://pytorch.org/docs/

### Academic Papers
1. **He et al.** - "Deep Residual Learning for Image Recognition" (CVPR 2016)
2. **Krizhevsky & Hinton** - "Learning Multiple Layers of Features from Tiny Images" (2009)
3. **Kreuzberger et al.** - "Machine Learning Operations (MLOps): Overview, Definition, and Architecture" (IEEE Access 2023)

### Datasets
- **CIFAR-10:** https://www.cs.toronto.edu/~kriz/cifar.html

---

## ğŸ“ Contact

- **Zahra ABOUHANE:** [abouhanezahra@gmail.com]
- **Yassine BAZGOUR:** [yassine.bazgour@gmail.com]
- **Repository:** https://github.com/yourusername/distributed-training-kubeflow-pipeline

---

## ğŸš€ Quick Start
```bash
# Clone repository
git clone https://github.com/ZahraAbouhane/distributed-training-kubeflow-pipeline.git
cd distributed-training-kubeflow-pipeline

# Start cluster
minikube start --memory=8000 --cpus=4

# Follow installation guide above
# Then execute pipeline:
kubectl create -f pipelines/real_pipeline.yaml
```

---

**Built with â¤ï¸ for MLOps learning and academic demonstration**